{"body":"## The Cut-Out\r\n\r\nA [\"Cut-Out\"](http://en.wikipedia.org/wiki/Cut-out_(espionage)), in espionage terms, is a trusted intermediary who facilitates the exchange of information between agents.\r\n\r\nThis project is a server and accompanying client library to provide an intermediary to store and communication information between browser-based applications (e.g., applications that use `localStorage` or `IndexedDB` as their primary storage).\r\n\r\n### Quick Start\r\n\r\nFirst, start the server.  It should work to run `python dev-server.py` - all the prerequesites are included in `vendor/`\r\n\r\nThis will start the server up at `http://localhost:8088/sync/` - see `-h` for other options.  Note that `test-server.py` is for running the tests, so you don't want that one.  You can also use `python dev-server.py my-app/` which will serve up the files in `my-app/` on the root and put the server in `/sync/` (and the client library in `/sync/syncclient.js`).\r\n\r\nNext, include this in your page:\r\n\r\n```html\r\n<script src=\"http://localhost:8088/sync/syncclient.js\"></script>\r\n<script src=\"https://browserid.org/include.js\"></script>\r\n```\r\n\r\nAnd create an object to integrate with your stored data:\r\n\r\n```javascript\r\nvar MyAppData = {\r\n  getPendingObjects: function (callback) {\r\n    var result = [];\r\n    for (object in objectsThatArentSaved) {\r\n      result.push({id: object.id, data: object});\r\n    }\r\n    for (id in objectsThatWereDeleted) {\r\n      result.push({id: id, deleted: true});\r\n    }\r\n    callback(result);\r\n  },\r\n  objectsSaved: function (objects) {\r\n    objects.forEach(function (object) {\r\n      if (object.deleted) {\r\n        confirmDelete(object.id);\r\n      } else {\r\n        confirmSaved(object.data);\r\n      }\r\n    });\r\n  },\r\n  objectsReceived: function (objects) {\r\n    objects.forEach(function (object) {\r\n      if (object.deleted) {\r\n        deleteObject(object.id);\r\n      } else {\r\n        createObject(object.data);\r\n      }\r\n    });\r\n  },\r\n  onupdate: function () {}\r\n};\r\nsync = new Sync(MyAppData);\r\nsync.watch({\r\n  onlogin: function (email) {\r\n    $('#login').text(email);\r\n  },\r\n  onlogout: function (email) {\r\n    $('#login').text('login');\r\n  }\r\n});\r\n$('#login').click(function () {\r\n  if ($('#login').text() == 'login') {\r\n    sync.request();\r\n  } else {\r\n    sync.logout();\r\n  }\r\n});\r\n\r\n// and call MyAppData.onupdate() whenever you update your objects\r\n```\r\n\r\nThat's it!  You also have a login system!\r\n\r\n### Library\r\n\r\nThe library is present in `syncclient.js`.  While there are several internals that you could poke around with, the \"simple\" version is just one function, `new Sync(appData, {options})` which returns a sync object.\r\n\r\n#### appData\r\n\r\nThis is the object that tells the sync service how to interact with your data.  This object has three important methods:\r\n\r\n##### `appData.getPendingObjects(callback)`\r\n\r\nThis should return all the *new* objects in your application.  You have to keep track yourself of when objects have been saved before, or if they've been updated, or if they've been deleted.\r\n\r\nYour function should call `callback(null, [objects])` (the objects could be an empty list).  The return value does not matter.\r\n\r\nEach object should look like:\r\n\r\n```javascript\r\n{\r\n  id: \"some stable id\",\r\n  type: \"type-of-object\",\r\n  data: {some data}\r\n}\r\n```\r\n\r\nThe `id` is something you should make yourself, but it should distinguish between updates and new objects.  Creating an ID with a UUID is perfectly fine, so long as you make that ID persistent.  The id should be a string or integer.\r\n\r\nThe `type` is just the kind of the object.  It should be a string.\r\n\r\nThe `data` is the actual data.  Some JSONable object.\r\n\r\nIf you have a deleted object, you should represent it like:\r\n\r\n```javascript\r\n{\r\n  id: \"some stable id\",\r\n  type: \"type-of-object\",\r\n  deleted: true\r\n}\r\n```\r\n\r\nNote that having returned these objects, they may not get used!\r\n\r\n\r\n##### `appData.objectsSaved(objects)`\r\n\r\nThis method indicates that the objects (as returned from `getPendingObjects` have actually been saved.  You should store something with the objects showing that they are saved, so you don't return them in future calls to `getPendingObjects`.\r\n\r\nNote that attempts to upload objects can fail, so the results of `getPendingObjects` may be discarded and retrieved later for another attempt.  Typically such a case will result in a sequence of getPendingObjects, objectsReceived, getPendingObjects again, and finally objectsSaved.\r\n\r\nNote if you use blobs, you should check for the `href` of the object, which will have been set after saving.  You can still keep the source of the blob around if you want, or remove it and rely on the remote/linked blob.\r\n\r\n##### `appData.objectsReceived(objects)`\r\n\r\nThis happens when a new object has appeared on the server.  The objects are formatted just as above, and probably came from another instance of your application.\r\n\r\nThese objects can include updates, new objects, and deletes.\r\n\r\n##### `appData.status(message)`\r\n\r\nThis not-really-documented optional method is called to indicate things that are happening with the sync service.  You could maybe use it to indicate the sync status in the UI somewhere.\r\n\r\n##### `appData.resetSaved()`\r\n\r\nThis is called when the server is switched out somehow, and you should erase all information about what objects have been saved in the past. This is done before starting a new from-scratch sync.\r\n\r\n##### `appData.reportObjectErrors()`\r\n\r\nThis optional method is called if `getPendingObjects` returns a bad object.\r\n\r\n##### `appData.onupdate`\r\n\r\nThis attribute will be set by the sync service, so long as the attribute exists (if you don't define it, it won't be set - even setting it to `null` is sufficient).  You are encouraged, but not required, to call this when the data changes.  This will cause the sync process to trigger soon, updating the server with the new data.\r\n\r\n\r\n#### options\r\n\r\nThere are a couple options to `Sync(appData, {options})`:\r\n\r\n`assertion`: a browerid/persona assertion to login with.  If you don't provide this, then the service is started in a potentially not-logged-in state and you may have to call `navigator.id.request()` to trigger a login.\r\n\r\n`appName`: this is the name of your app.  This is important only if your domain has multiple distinct apps.\r\n\r\n\r\n#### Sync object\r\n\r\nThe object returned as some methods:\r\n\r\n`lastSyncTime()`: returns the timestamp (milliseconds) of the last successful sync.\r\n\r\n`scheduleImmediately()`: try to do a sync right away.\r\n\r\n`resetSchedule()`: reset the period polling to a normal pace.\r\n\r\n`scheduleSlowly()`: slow down the polling.\r\n\r\n`activate()/deactivate()`: activate or deactivate the scheduler. Before you retire a sync object you should call `sync.deactivate()`. The scheduler is automatically activated on startup.  The polling frequency is automatically adjusted based on whether the tab is in the background or not.\r\n\r\n`request()`: ask to get authentication (this is identicaly to `navigator.id.request()`)\r\n\r\n`logout()`: get rid of all authentication information.\r\n\r\n`toggleLogin()`: if logged in, then log out.  If logged out, then request login.\r\n\r\n`watch(options)`: similar to `navigator.id.watch()` but is called after server verification is done.  The `onlogin` method is called with `onlogin(email, completeData)` (not an assertion).  Note this uses cached credentials, unlike `navigator.id.watch()`.\r\n\r\n`reset(callback)`: forgets all information about synchronization. This doesn't get rid of anything on the server, but does cause the next sync to start from scratch.\r\n\r\n`authentiateUrl(url)`: takes a URL (for a `blob.href`) and adds authenticating information to it.\r\n\r\n### Protocol\r\n\r\n#### Expectations\r\n\r\nThe model of sync is a stream of updates.  All clients both put their local updates into this stream, and read the collective stream. Everything has to be represented as a concrete item in the stream, meaning that delete actions are also present in the stream.\r\n\r\nThere is no conflict resolution, so clients must make sure they do not overwrite each other's updates.  If a conflict cannot be resolved without interaction (e.g., simple overwrite is not considered acceptable, and automatic merging is not possible) then it must be possible to represent the conflicted state directly, and at some point some client can resolve the conflict (possibly with user interaction) and put the unconflicted object into the stream.\r\n\r\nThe stream is ordered, along a single timeline.  The timeline markers should *not* be seen as based on any time or clock, as this leads to confusion and it's not clear whose \"now\" we are talking about. Instead the server has a counter, and all clients work from that counter. (The counter need not be an uninterrupted stream of integers, just increasing.)\r\n\r\nAll interaction between client and server should happen without user intervention.  Everything is expected to be highly asynchronous, and the server may reject requests or be unavailable for short periods of time, and this should not affect user experience.\r\n\r\nWe expect for a new client to be able to create a good-enough duplicate of the data in other clients.  \"Good-enough\" because some data might be kept by clients but expired by the server because it was marked as not being permanently interesting.\r\n\r\n**TODO:** For \"known\" datatypes the sync server ensures the integrity of data, according to the most up-to-date notion of correctness for the data type.  As such the sync server must be updated frequently, but clients will be protected from some other rogue clients. ('''Note:''' not sure if this is a practical expectation?)\r\n\r\nWe'll go out-of-order time-wise, and forget about authentication for now.\r\n\r\nEverything happens at a single URL endpoint, we'll call it `/USER`\r\n\r\n#### Objects\r\n\r\nEach object looks like this:\r\n\r\n```javascript\r\n{type: \"type_name\",\r\n id: \"unique identifier among objects of this type\",\r\n expires: timestamp,\r\n data: {the thing itself}\r\n}\r\n```\r\n\r\nNote the `data` can be any JSONable object, including a string.\r\n\r\n**TODO:** The `expires` key is entirely optional, and allows the server to delete the item (if it has not otherwise been updated).\r\n\r\nThe `id` key must be unique for the type (submitting another key by the same id means that you are overwriting that object).\r\n\r\nYou can also have a deleted object, which lives as an object in sync but doesn't have any data:\r\n\r\n```javascript\r\n{type: \"type_name\",\r\n id: \"unique identifier\",\r\n expires: timestamp,\r\n deleted: true\r\n}\r\n```\r\n\r\n#### Requests\r\n\r\nYou can retrieve and send updates.  The first time is simple, you just want to accept whatever the server has: can just do:\r\n\r\n    GET /USER\r\n\r\nThis returns the response document:\r\n\r\n```javascript\r\n{collection_id: \"string_id\",\r\n objects: [[counter1, object1], [counter2, object2]]\r\n}\r\n```\r\n\r\nThe `collection_id` key is only in there because it was not sent with the request; it's a kind of \"hello\".\r\n\r\nSubsequent requests look like:\r\n\r\n    GET /USER?since=counter2&collection_id=string_id\r\n\r\nYou get the objects back, but with no `collection_id` (you already know it!)  If there have been no changes you get back a `204 No Content` response.\r\n\r\nIf `objects` is empty, you start with a counter `0`.\r\n\r\nIf the collection has changed, and your `string_id` doesn't match the server anymore, then you'll get:\r\n\r\n```javascript\r\n{collection_changed: true,\r\n collection_id: \"new_id\",\r\n objects: [[counter1, ...], ...]\r\n}\r\n```\r\n\r\nYou should then forget your remembered `since` value and all the updates you have sent to the server.  This signals that whatever server or data you were communicating with before is gone.\r\n\r\nWhen you have updates you want to send, you do:\r\n\r\n    POST /USER?since=counter2&collection_id=string_id\r\n\r\n    [{id: \"my obj1\", type: \"thingy\", data: {...}, ...]\r\n\r\nThis may return a `collection_changed` error, but also there may have been an update since you last retrieved objects.  This will not do! The `since=counter2` shows when you last did a GET. If there have been updates you get a new GET-like response:\r\n\r\n```javascript\r\n{since_invalid: true,\r\n objects: [[counter3, object]]\r\n}\r\n```\r\n\r\nYou should incorporate the new object (which might conflict some with your own objects, which is why we do all this!), and then resubmit the request:\r\n\r\n    POST /USER?since=counter3&collection_id=string_id\r\n    ...\r\n\r\nA successful response will be:\r\n\r\n```javascript\r\n{object_counters: [counter4, counter5, ...]}\r\n```\r\n\r\nThe counters will correspond to each item that you sent.  You should keep the highest counter as your `since` value.  (**Note:** maybe this should include a timestamp of sorts too?)\r\n\r\n\r\n\r\n##### Conflicts\r\n\r\nWe do not resolve conflicts as part of sync, and you are strongly recommended not to burden your users with conflicts as part of your sync schedule.\r\n\r\nIn some cases you can resolve conflicts yourself.  For instance, if the data is not very interesting, you can just choose a winner.\r\n\r\nIf you can't automatically resolve the conflicts you must incorporate all your conflicting edits into a new object, and when the user at some point can attend to the object you can show them the conflicts and ask for a resolution, putting the resolved object onto the server.\r\n\r\n\r\n##### Partial Results\r\n\r\nYou may not want too many results.  In this case add to your GET requests:\r\n\r\n    GET /USER?...&limit=10\r\n\r\nThis will return at most 10 items.  The server may also choose not to return a full set of items.  In either case the result object will have `incomplete: true`.  You can make another request and get more items.\r\n\r\n##### Typed Results\r\n\r\nSometimes you only care about a subset of objects.  The stream can have any number of types of objects, and while a full client may handle everything a more limited client may not care about some items. In this case do:\r\n\r\n    GET /USER?...&include=type1&include=type2\r\n\r\nThis gives you only `type1` and `type2` objects. Instead of opting in to some objects, you can also opt-out with `exclude=type1&exclude=type2`.\r\n\r\nThe response may include `until: \"counter3\"`, which might be newer than the newest item that was returned (this happens when the newest item is not of the type you requested).\r\n\r\nYou may also include these same filters on your POST requests; this keeps a conflict from happening even if an object of an excluded type has been added.\r\n\r\n\r\n#### Server Failure and Backoff\r\n\r\nThe server may return a 503 response, with a `Retry-After` value.  In any request it may also reply with `X-Sync-Poll-Time`, which is appended to a successful request but requests that you not make another request for the given time (in seconds).\r\n\r\n#### Authentication\r\n\r\nEach request has to have authentication.  The authentication uses BrowserID.  To get authentication information you make a request to:\r\n\r\n    POST /DB/verify\r\n\r\n    assertion=...&audience=...\r\n\r\nThis will return a JSON response that will indicate how to authenticate future requests, like:\r\n\r\n```javascript\r\n{\r\n  \"email\": \"user@example.com\",\r\n  \"auth\": {\r\n    \"query\": {\"auth\": \"auth_string\"}\r\n  }\r\n}\r\n```\r\n\r\nWhat is in the `\"auth\"` key determines what you should do to authenticate future requests.  401 responses indicate you should re-authenticate with a new assertion.\r\n\r\n## Clients\r\n\r\nThe client algorithm is to get and put updates, storing them locally. That easy?  Sure!\r\n\r\n#### GET\r\n\r\nThe client needs to keep a record of these values:\r\n\r\n* The `since` counter\r\n* The `collection_id`\r\n* Authentication\r\n\r\nThe `since` counter and the `collection_id` go together to point to where in the stream of updates the client is.  If the collection changes, that counter becomes meaningless, hence the `collection_id` - and when you get a `collection_changed` response you should forget your `since` value.\r\n\r\nEvery time you get a response, you update the `since` value if there were updates.  If `until` is set on the response, use that, otherwise use the counter from the last item in `objects`.  If you get no objects, no until, or a 204 No Content respones, then don't change anything.\r\n\r\nYou should keep getting stuff so long as the response includes `incomplete: true`.  Also Retry-After and X-Sync-Poll-Time should inform the speed at which you make requests.\r\n\r\n#### POST\r\n\r\nOnce you've retrieved the values, then you can send your own new values.  You'll send `?since={since}` just like with GET, because you must always incorporate every value before sending your own.  This ensures that anyone who adds to the sync timeline is fully aware of everything preceding.\r\n\r\nThe POST results, when successful, also update `since`.  And the POST results when unsuccessful look just like a GET (since you needed to do a GET, right?)\r\n\r\n##### Quarantine\r\n\r\nSometimes you may not understand an object you receive; its type, or the format it is in.  This might be because of corruption, but it might also be because another client has a newer/richer notion of the type than you do.\r\n\r\n#### Blobs\r\n\r\nSometimes you will want to store large amount of data with an object, for instance an image.\r\n\r\nYou could keep the image (encoded in some fashion, perhaps as a `data:` URL) stored in the objects themselves.  The problem is that clients *must* download all objects, and so the update process would requiring downloading the whole image, and would require downloading it during what might otherwise be a simple update, or before adding objects.  The sync model essentially keeps you from doing lazy fetching - something which is usually okay, but not always.\r\n\r\nInstead of storing the object directly in the storage, you can POST an update with the complete blob (in addition to any metadata associated with that blob) and the blob will be stored at a separate URL.  This blob will be managed alongside the object (i.e., updated and deleted with the object).\r\n\r\nTo do this, use an object like:\r\n\r\n```javascript\r\n{\r\n  type: \"image\",\r\n  id: \"pic1\",\r\n  data: {\r\n    \"description\": \"A picture of me and Jim\"\r\n  },\r\n  blob: {\r\n    content_type: \"image/jpeg\",\r\n    data: \"base64 encoded image\"\r\n  }\r\n}\r\n```\r\n\r\nWhen `AppData.objectsSaved()` is called you'll get back something like:\r\n\r\n```javascript\r\n{\r\n  type: \"image\",\r\n  id: \"pic1\",\r\n  data: {\r\n    \"description\": \"A picture of me and Jim\"\r\n  },\r\n  blob: {\r\n    content_type: \"image/jpeg\",\r\n    href: \"http://storage/sync/domain/user/+static/4jD19Fde-D134\"\r\n  }\r\n}\r\n```\r\n\r\nThe URL will still be protected by authentication.  To use the URL you must do `url = sync.authenticateUrl(object.blob.href)`\r\n\r\nIf you make an update you can (and must!) keep the `blob: {content_type: ..., href: ...}` portion of the object; however, you do not need to upload new blob data with each update.\r\n\r\nNote the URL will be controlled by CORS headers, so you can access its content with an XMLHttpRequest.\r\n\r\n### To Do\r\n\r\n* Implement periodic garbage collection\r\n\r\n* Move static file hosting to another domain (though that's more of a deployment concern).\r\n\r\n* Allow public blobs\r\n\r\n* Allow first sync to go from most-recent to oldest, instead of the other way around","note":"Don't delete this file! It's used internally to help with page regeneration.","name":"The Cut-Out","google":"UA-6731441-13","tagline":"A data storage and synchronozation system for client/browser-based applications"}